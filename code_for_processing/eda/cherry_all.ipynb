{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import TypedDict\n",
    "\n",
    "import chardet\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from meteostat import Daily, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = Path.cwd().parents[1] / \"datasets/csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncodingResult(TypedDict):\n",
    "    encoding: str | None\n",
    "    confidence: float\n",
    "    language: str | None\n",
    "\n",
    "\n",
    "def check_encoding(file_path: str) -> str | None:\n",
    "    \"\"\"Detects the encoding of a file.\"\"\"\n",
    "    with open(file=file_path, mode=\"rb\") as f:\n",
    "        result: EncodingResult = chardet.detect(byte_str=f.read())\n",
    "    return result[\"encoding\"]\n",
    "\n",
    "\n",
    "def convert_to_utf8(file_path: str, encoding: str | None) -> None:\n",
    "    \"\"\"Converts a file to UTF-8 encoding.\"\"\"\n",
    "    # Open the file with the detected encoding\n",
    "    with open(file=file_path, mode=\"r\", encoding=encoding) as f:\n",
    "        lines: list[str] = f.readlines()\n",
    "\n",
    "    # Remove the line that starts with 'prefix:'\n",
    "    lines = [line for line in lines if not line.startswith(\"prefix:\")]\n",
    "\n",
    "    # Write the lines back out in UTF-8\n",
    "    with open(file=file_path, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_encoding(str(dataset_path/\"meteoswiss.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read and concatenate CSV files\n",
    "df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(dataset_path/\"washingtondc.csv\"),\n",
    "        pd.read_csv(dataset_path/\"liestal.csv\"),\n",
    "        pd.read_csv(dataset_path/\"kyoto.csv\"),\n",
    "        pd.read_csv(dataset_path/\"vancouver.csv\"),\n",
    "        pd.read_csv(dataset_path/\"south_korea.csv\"),\n",
    "        pd.read_csv(dataset_path/\"japan.csv\"),\n",
    "        pd.read_csv(dataset_path/\"nyc.csv\"),\n",
    "        pd.read_csv(dataset_path/\"meteoswiss.csv\", encoding=\"ISO-8859-1\"),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to split location \n",
    "def split_location(location): \n",
    "    parts = location.split('/', 1) \n",
    "    if len(parts) == 2: return parts \n",
    "    else: return [None, location]\n",
    "    \n",
    "# Apply the function to the location column \n",
    "df[['country', 'city']] = df['location'].apply(split_location).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop duplicates based on the city column \n",
    "unique_cities_df = df.drop_duplicates(subset=['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_cities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Initialize an empty list to store weather data for all cities\n",
    "all_weather_data = []\n",
    "\n",
    "# Step 2: Loop through each row (city) in unique_cities_df to fetch temperature data\n",
    "for _, row in unique_cities_df.iterrows():\n",
    "    city = row[\"city\"]\n",
    "    lat = row[\"lat\"]\n",
    "    long = row[\"long\"]\n",
    "\n",
    "    # Check if lat and long are strings and fix sneaky incorrect unicode character pretending to be a minus if necessary\n",
    "    if isinstance(lat, str):\n",
    "        lat = float(lat.replace(\"\\u2013\", \"-\"))\n",
    "    if isinstance(long, str):\n",
    "        long = float(long.replace(\"\\u2013\", \"-\"))\n",
    "\n",
    "    # Create a Meteostat Point using latitude and longitude\n",
    "    location = Point(lat, long)\n",
    "\n",
    "    # Define the date range (start_date to end_date)\n",
    "    start_date = datetime(2023, 1, 1)  # Full year example\n",
    "    end_date = datetime(2023, 12, 31)\n",
    "\n",
    "    # Fetch daily weather data for the date range\n",
    "    weather_data = Daily(location, start_date, end_date).fetch()\n",
    "\n",
    "    if not weather_data.empty:\n",
    "        # Reset index to bring the date (time) into the DataFrame as a regular column\n",
    "        weather_data.reset_index(inplace=True)\n",
    "\n",
    "        # Add columns for city and day_of_year\n",
    "        weather_data[\"day_of_year\"] = weather_data[\"time\"].dt.dayofyear\n",
    "        weather_data[\"city\"] = city\n",
    "        \n",
    "        # Append the cleaned data to the list\n",
    "        all_weather_data.append(weather_data[[\"time\", \"city\", \"day_of_year\", \"tavg\", \"tmin\", \"tmax\"]])\n",
    "\n",
    "# Step 3: Concatenate all fetched data into a single DataFrame\n",
    "weather_data_df = pd.concat(all_weather_data, ignore_index=True)\n",
    "\n",
    "weather_data_df.to_csv(\"all_dates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"all_dates.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform a left join based on 'city' and 'time' (full date)\n",
    "merged_df = pd.merge(\n",
    "    df2,\n",
    "    df,\n",
    "    left_on=[\"city\", \"time\"],\n",
    "    right_on=[\"city\", \"bloom_date\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_temp\", \"_bloom\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df['time'] = pd.to_datetime(merged_df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the year from the 'time' column for plotting\n",
    "merged_df[\"temp_year\"] = merged_df[\"time\"].dt.year\n",
    "\n",
    "# Group data by city and temp_year (derived from time)\n",
    "city_year_groups = merged_df.groupby([\"city\", \"temp_year\"])\n",
    "\n",
    "# Loop through each city-year combination\n",
    "for (city, year), data in city_year_groups:\n",
    "    if data.empty:\n",
    "        continue\n",
    "\n",
    "    # Drop rows where tavg is NaN\n",
    "    city_year_data = data.dropna(subset=[\"tavg\"])\n",
    "\n",
    "    # Plot temperature over the year\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(\n",
    "      city_year_data[\"day_of_year\"], \n",
    "      city_year_data[\"tavg\"], \n",
    "      label=\"Daily Avg Temp\", \n",
    "      color=\"blue\"\n",
    "    )\n",
    "    plt.plot(\n",
    "      city_year_data[\"day_of_year\"], \n",
    "      city_year_data[\"tmin\"], \n",
    "      label=\"Daily Min Temp\", \n",
    "      color=\"green\"\n",
    "    )\n",
    "    plt.plot(\n",
    "      city_year_data[\"day_of_year\"], \n",
    "      city_year_data[\"tmax\"], \n",
    "      label=\"Daily Max Temp\", \n",
    "      color=\"orange\"\n",
    "    )\n",
    "\n",
    "    # Find the bloom date for this city and year (if available)\n",
    "    bloom_data = data.dropna(subset=[\"bloom_doy\"])\n",
    "    if not bloom_data.empty:\n",
    "        bloom_doy = bloom_data[\"bloom_doy\"].values[0]\n",
    "        bloom_temp = city_year_data.loc[\n",
    "            city_year_data[\"day_of_year\"] == bloom_doy, \"tavg\"\n",
    "        ].values\n",
    "\n",
    "        if len(bloom_temp) > 0:\n",
    "            bloom_temp = bloom_temp[0]\n",
    "            # Add marker for bloom day\n",
    "            plt.scatter(\n",
    "                bloom_doy,\n",
    "                bloom_temp,\n",
    "                color=\"red\",\n",
    "                s=100,\n",
    "                zorder=5,\n",
    "                label=\"Peak Bloom\"\n",
    "            )\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title(f\"Temperature and Peak Bloom in {city.capitalize()}, {year}\")\n",
    "    plt.xlabel(\"Day of Year\")\n",
    "    plt.ylabel(\"Average Temperature (Â°C)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
