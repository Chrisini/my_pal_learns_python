{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb4ffbf-7a23-472b-8a53-ce5467325e50",
   "metadata": {},
   "source": [
    "# Blossom\n",
    "* convert the R tutorial to python\n",
    "* https://realworlddatascience.net/ideas/tutorials/posts/2023/04/13/flowers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be1e58b7-0c60-4ade-859c-902f8a4d0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import plotly.express as px\n",
    "#import plotly.graph_objects as go\n",
    "import requests\n",
    "#from bs4 import BeautifulSoup\n",
    "\n",
    "# https://gitlab.cicsnc.org/jared/ghcnd-search\n",
    "# from rnoaa import GHCNData # need the python version of the r thingi ...\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ade11a-33f3-4711-8fd2-48c164aa7cd7",
   "metadata": {},
   "source": [
    "# Create data\n",
    "\n",
    "R version\n",
    "```{r}\n",
    "lilac <-                   \n",
    "  tibble(month = c(\"May\", \"April\", \"April\", \"April\", \"April\", \"April\", \"May\", \n",
    "                   \"April\", \"May\", \"April\", \"May\", \"April\", \"May\", \"May\"),\n",
    "         day   =  c(10, 28, 24, 28, 20, 25, 13, 12, 9, 21, 2, 30, 1, 12),\n",
    "         year  = 1839:1852,\n",
    "         date  = as.Date(paste(month, day, year), format = \"%B %d %Y\"),\n",
    "         doy   = parse_number(format(date, \"%j\"))) \n",
    "\n",
    "lilac %>% \n",
    "  kable(align = \"c\",\n",
    "        caption = \"Table 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\") %>%\n",
    "  kable_styling() %>%\n",
    "  scroll_box(width = \"100%\", height = \"400px\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e304c636-785b-4459-b495-d4f69c18fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python translation\n",
    "\n",
    "lilac_data = {\n",
    "    'month': [\"May\", \"April\", \"April\", \"April\", \"April\", \"April\", \"May\", \n",
    "              \"April\", \"May\", \"April\", \"May\", \"April\", \"May\", \"May\"],\n",
    "    'day': [10, 28, 24, 28, 20, 25, 13, 12, 9, 21, 2, 30, 1, 12],\n",
    "    'year': list(range(1839, 1853))\n",
    "}\n",
    "\n",
    "lilac_df = pd.DataFrame(lilac_data)\n",
    "lilac_df['date'] = pd.to_datetime(lilac_df['month'] + ' ' + lilac_df['day'].astype(str) + ' ' + lilac_df['year'].astype(str), format='%B %d %Y')\n",
    "lilac_df['doy'] = lilac_df['date'].dt.dayofyear\n",
    "\n",
    "#display(HTML(\"<h3>Table 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.</h3>\"))\n",
    "#display(lilac_df.style.set_table_attributes(\"style='width:100%; height:400px;'\").set_caption(\"Table 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40a28ec-4700-4326-bab2-106f30ff39bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>doy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May</td>\n",
       "      <td>10</td>\n",
       "      <td>1839</td>\n",
       "      <td>1839-05-10</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April</td>\n",
       "      <td>28</td>\n",
       "      <td>1840</td>\n",
       "      <td>1840-04-28</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April</td>\n",
       "      <td>24</td>\n",
       "      <td>1841</td>\n",
       "      <td>1841-04-24</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>April</td>\n",
       "      <td>28</td>\n",
       "      <td>1842</td>\n",
       "      <td>1842-04-28</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April</td>\n",
       "      <td>20</td>\n",
       "      <td>1843</td>\n",
       "      <td>1843-04-20</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  day  year       date  doy\n",
       "0    May   10  1839 1839-05-10  130\n",
       "1  April   28  1840 1840-04-28  119\n",
       "2  April   24  1841 1841-04-24  114\n",
       "3  April   28  1842 1842-04-28  118\n",
       "4  April   20  1843 1843-04-20  110"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lilac_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088fb060-5800-41f3-b273-86510ecd7b30",
   "metadata": {},
   "source": [
    "# Global Historical Climatology Network daily weather data from NOAA's FTP server\n",
    "* https://gis.ncdc.noaa.gov/arcgis/rest/services/cdo/ghcnd/MapServer\n",
    "\n",
    "* \"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNOW\",\"SNOW_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"PGTM\",\"PGTM_ATTRIBUTES\",\"WDFG\",\"WDFG_ATTRIBUTES\",\"WSFG\",\"WSFG_ATTRIBUTES\",\"WT03\",\"WT03_ATTRIBUTES\",\"WT08\",\"WT08_ATTRIBUTES\",\"WT16\",\"WT16_ATTRIBUTES\"\n",
    "\n",
    "\n",
    "* A RESTful web service is a simple web based service which is implemented via HTTP and the basic principles of REST. Access to data is available via simple URLs instead of special programming for database access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b761cc-b8cc-4511-8fb8-570fcc95bbac",
   "metadata": {},
   "source": [
    "## R version\n",
    "```{r}\n",
    "temp <- \n",
    "  ghcnd_search(stationid = \"BE000006447\",\n",
    "               var = c(\"tmax\", \"tmin\"),\n",
    "               date_min = \"1839-01-01\",\n",
    "               date_max = \"1852-12-31\") %>%\n",
    "  reduce(left_join) %>%\n",
    "  transmute(year = parse_number(format(date, \"%Y\")), \n",
    "            date, \n",
    "            tmax = tmax / 10, \n",
    "            tmin = tmin / 10, \n",
    "            temp = (tmax + tmin) / 2)\n",
    "  \n",
    "temp %>% \n",
    "  kable(align = \"c\", \n",
    "        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\", \n",
    "                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n",
    "        caption = \"Table 2: Temperature observed at Brussels Observatory between 1839 and 1852.\") %>%\n",
    "  kable_styling() %>%\n",
    "  scroll_box(width = \"100%\", height = \"400px\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c05680-81de-496e-863c-2ad84ccfad3b",
   "metadata": {},
   "source": [
    "## Python translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d6b42ec0-187b-4fc6-ad09-898141fbe1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1fc3ad09-b79d-4f3a-8c51-771666dc97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_gen(api_url, length = 1, token=None):\n",
    "    response = requests.get(api_url, headers={'token':token})\n",
    "\n",
    "    j = response.json()\n",
    "\n",
    "        \n",
    "    #print(j)\n",
    "    df = pd.DataFrame.from_dict(j['results'])\n",
    "    \n",
    "    #df = pd.DataFrame([[d['v'] for d in x['c']] for x in j['rows']], columns=[d['label'] for d in j['cols']])\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    response_short = \"\"\n",
    "    for i_r, r in enumerate(response):\n",
    "        response_short = response_short + str(r)\n",
    "        if i_r >length:\n",
    "            break\n",
    "    return response_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d531d6-41ec-4e8d-9b68-f09a3a49b1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1d5b8-02dc-4a83-995a-bdf20a43d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives me the overview page as json\n",
    "api_url = \"https://gis.ncdc.noaa.gov/arcgis/rest/services/cdo/ghcnd/MapServer?f=pjson\" # \"https://jsonplaceholder.typicode.com/todos/1\"\n",
    "\n",
    "print(response_gen(api_url), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331bacb-e2ea-4b05-a2db-9c9d1e106db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/all?stationid=BE000006447&date_min=1839-01-01&date_max=1852-12-31&var=tmax\"\n",
    "response = requests.get(api_url)\n",
    "\n",
    "print(response_gen(api_url), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b441d72-ff9d-46db-b5eb-477b39b16f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\\n<html>\\n <head>\\n  <title>Index of /data/global-historical-climatology-net'b'work-daily/access</title>\\n </head>\\n <body>\\n<h1>Index of /data/global-historical-climatology-network-daily/access</h1>\\n  <table>\\n'b'   <tr><th><a href=\"?C=N;O=D\">Name</a></th><th><a href=\"?C=M;O=A\">Last modified</a></th><th><a href=\"?C=S;O=A\">Size</a></th><th>' 100\n"
     ]
    }
   ],
   "source": [
    "# shows me all the csv files\n",
    "# https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/\n",
    "api_url = \"https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access\" # \"https://jsonplaceholder.typicode.com/todos/1\"\n",
    "print(response_gen(api_url), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fd2cd825-5875-4f71-97c4-8b5014e91f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    972\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\simplejson\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, **kw)\u001b[0m\n\u001b[0;32m    524\u001b[0m             and not use_decimal and not kw):\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\simplejson\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\simplejson\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[0;32m    399\u001b[0m                 \u001b[0midx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-8a887dd0ae87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# https://stackoverflow.com/questions/25856253/flatten-and-expand-a-csv-file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# stationid=BE000006447\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-111-c66d14ad6ac9>\u001b[0m in \u001b[0;36mresponse_gen\u001b[1;34m(api_url, length, token)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'token'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m             \u001b[1;31m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m             \u001b[1;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "api_url = \" https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&stationid=GHCND:BE000006447&startdate=2023-01-01&enddate=2023-12-31\"\n",
    "token = \"UyFbHbyzqdoSyGJtCJrWlSjZRDlQDEPN\"\n",
    "\n",
    "# https://stackoverflow.com/questions/25856253/flatten-and-expand-a-csv-file\n",
    "\n",
    "print(response_gen(api_url, 1, token=token)) # stationid=BE000006447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b73003d9-36fe-4b75-8253-2fac6faccd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"errorCode\":400,\"errorMessage\":\"Bad Request\",\"errors\":[{\"field\":\"dataset\",\"message\":\"Unsupported dataset.\",\"value\":\"global-hist'b'orical-climatology-network\"}]}'\n"
     ]
    }
   ],
   "source": [
    "api_url = \"https://www.ncei.noaa.gov/access/services/data/v1?dataset=global-historical-climatology-network\" # &dataTypes=SNOW&stations=ACW00011604&startDate=1949-01-01&endDate=1949-01-06\"\n",
    "print(response_gen(api_url, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be49ace0-8fe6-4e04-b555-7ee76025d066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# only hourly\n",
    "# https://www.ncei.noaa.gov/access/services/search/v1/datasets\n",
    "api_url= \"https://www.ncei.noaa.gov/access/services/data/v1?dataset=global-historical-climatology-network-hourly&stations=BE000006447&startDate=1800-01-01&endDate=1880-12-31\"\n",
    "print(response_gen(api_url, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac377c17-50f7-414a-8443-65b69cde157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"WIND_DIR\",\"WIND_SPEED\"\\n\"AUCE\",\"2016-01-01T00:09:00\",\"-20.20\",\"57.50\",\"90\",\"40\"\\n\"AUCE\",\"'b'2016-01-01T21:09:00\",\"-20.20\",\"57.50\",\"20\",\"20\"\\n\"AUCE\",\"2016-01-02T00:09:00\",\"-20.20\",\"57.50\",\"350\",\"20\"\\n\"AUCE\",\"2016-01-02T03:0'b'9:00\",\"-20.20\",\"57.50\",\"340\",\"20\"\\n\"AUCE\",\"2016-01-02T06:09:00\",\"-20.20\",\"57.50\",\"350\",\"30\"\\n\"AUCE\",\"2016-01-02T09:09:00\",\"-20.20\"'b',\"57.50\",\"330\",\"30\"\\n\"AUCE\",\"2016-01-02T12:09:00\",\"-20.20\",\"57.50\",\"330\",\"40\"\\n\"AUCE\",\"2016-01-02T15:09:00\",\"-20.20\",\"57.50\",\"330\"'b',\"10\"\\n\"AUCE\",\"2016-01-02T18:09:00\",\"-20.20\",\"57.50\",\"300\",\"30\"\\n\"AUCE\",\"2016-01-02T21:09:00\",\"-20.20\",\"57.50\",\"320\",\"20\"\\n'\n"
     ]
    }
   ],
   "source": [
    "api_url= \"https://www.ncei.noaa.gov/access/services/data/v1?dataset=global-marine&dataTypes=WIND_DIR,WIND_SPEED&stations=AUCE&startDate=2016-01-01&endDate=2016-01-02&boundingBox=90,-180,-90,180\"\n",
    "print(response_gen(api_url, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f99a0a3-eb25-48d3-94b3-c36a2a8e60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ...?dataset=daily-summaries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# global-historical-climatology-network-daily/\n",
    "\n",
    "\n",
    "# /access/services/data/v1\n",
    "\n",
    "# &format=json\n",
    "\n",
    "\n",
    "#print(\"here\")\n",
    "\n",
    "#for r in response.text:\n",
    "#    print(r)\n",
    "\n",
    "#print(response)\n",
    "\n",
    "#print(response.text)\n",
    "\n",
    "# response.json()\n",
    "# {'userId': 1, 'id': 1, 'title': 'delectus aut autem', 'completed': False}\n",
    "\n",
    "\n",
    "# response.csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25db57b2-280b-4c83-b86e-bde93370f0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n \"currentVersion\": 11.1,\\n \"cimVersion\": \"3.1.0\",\\n \"serviceDescription\": \"\",\\n \"mapName\": \"Layers\",\\n \"description\": \"\",\\n \"copyrightText\": \"\",\\n \"supportsDynamicLayers\": true,\\n \"layers\": [\\n  {\\n   \"id\": 0,\\n   \"name\": \"GHCN Daily\",\\n   \"parentLayerId\": -1,\\n   \"defaultVisibility\": true,\\n   \"subLayerIds\": null,\\n   \"minScale\": 0,\\n   \"maxScale\": 0,\\n   \"type\": \"Feature Layer\",\\n   \"geometryType\": \"esriGeometryPoint\",\\n   \"supportsDynamicLegends\": true\\n  }\\n ],\\n \"tables\": [],\\n \"spatialReference\": {\\n  \"wkt\": \"GEOGCS[\\\\\"Longitude / Latitude (NAD 83)\\\\\",DATUM[\\\\\"NAD 83\\\\\",SPHEROID[\\\\\"GRS 80\\\\\",6378137.0,298.257222101]],PRIMEM[\\\\\"Greenwich\\\\\",0.0],UNIT[\\\\\"Decimal_Degree\\\\\",0.0174532925199433]]\",\\n  \"xyTolerance\": 8.983152841195212E-9,\\n  \"zTolerance\": 0.001,\\n  \"mTolerance\": 0.001,\\n  \"falseX\": -400,\\n  \"falseY\": -400,\\n  \"xyUnits\": 1.125899906842624E13,\\n  \"falseZ\": -100000,\\n  \"zUnits\": 10000,\\n  \"falseM\": -100000,\\n  \"mUnits\": 10000\\n },\\n \"singleFusedMapCache\": false,\\n \"initialExtent\": {\\n  \"xmin\": -222.31686648847815,\\n  \"ymin\": -197.00990330030845,\\n  \"xmax\": 222.31686648847815,\\n  \"ymax\": 197.00990330030845,\\n  \"spatialReference\": {\\n   \"wkt\": \"GEOGCS[\\\\\"Longitude / Latitude (NAD 83)\\\\\",DATUM[\\\\\"NAD 83\\\\\",SPHEROID[\\\\\"GRS 80\\\\\",6378137.0,298.257222101]],PRIMEM[\\\\\"Greenwich\\\\\",0.0],UNIT[\\\\\"Decimal_Degree\\\\\",0.0174532925199433]]\",\\n   \"xyTolerance\": 8.983152841195212E-9,\\n   \"zTolerance\": 0.001,\\n   \"mTolerance\": 0.001,\\n   \"falseX\": -400,\\n   \"falseY\": -400,\\n   \"xyUnits\": 1.125899906842624E13,\\n   \"falseZ\": -100000,\\n   \"zUnits\": 10000,\\n   \"falseM\": -100000,\\n   \"mUnits\": 10000\\n  }\\n },\\n \"fullExtent\": {\\n  \"xmin\": -180,\\n  \"ymin\": -90,\\n  \"xmax\": 180,\\n  \"ymax\": 90,\\n  \"spatialReference\": {\\n   \"wkt\": \"GEOGCS[\\\\\"Longitude / Latitude (NAD 83)\\\\\",DATUM[\\\\\"NAD 83\\\\\",SPHEROID[\\\\\"GRS 80\\\\\",6378137.0,298.257222101]],PRIMEM[\\\\\"Greenwich\\\\\",0.0],UNIT[\\\\\"Decimal_Degree\\\\\",0.0174532925199433]]\",\\n   \"xyTolerance\": 8.983152841195212E-9,\\n   \"zTolerance\": 0.001,\\n   \"mTolerance\": 0.001,\\n   \"falseX\": -400,\\n   \"falseY\": -400,\\n   \"xyUnits\": 1.125899906842624E13,\\n   \"falseZ\": -100000,\\n   \"zUnits\": 10000,\\n   \"falseM\": -100000,\\n   \"mUnits\": 10000\\n  }\\n },\\n \"datesInUnknownTimezone\": false,\\n \"minScale\": 0,\\n \"maxScale\": 0,\\n \"units\": \"esriDecimalDegrees\",\\n \"supportedImageFormatTypes\": \"PNG32,PNG24,PNG,JPG,DIB,TIFF,EMF,PS,PDF,GIF,SVG,SVGZ,BMP\",\\n \"documentInfo\": {\\n  \"Title\": \"GHCN Daily\",\\n  \"Author\": \"\",\\n  \"Comments\": \"\",\\n  \"Subject\": \"\",\\n  \"Category\": \"\",\\n  \"Version\": \"3.0.0\",\\n  \"AntialiasingMode\": \"Fast\",\\n  \"TextAntialiasingMode\": \"Force\",\\n  \"Keywords\": \"\"\\n },\\n \"capabilities\": \"Map,Query,Data\",\\n \"supportedQueryFormats\": \"JSON, geoJSON, PBF\",\\n \"exportTilesAllowed\": false,\\n \"referenceScale\": 0.0,\\n \"supportsDatumTransformation\": true,\\n \"archivingInfo\": {\"supportsHistoricMoment\": false},\\n \"supportsClipping\": true,\\n \"supportsSpatialFilter\": true,\\n \"supportsTimeRelation\": true,\\n \"supportsQueryDataElements\": true,\\n \"mapUnits\": {\"uwkid\": 9102},\\n \"maxRecordCount\": 1000,\\n \"maxImageHeight\": 4096,\\n \"maxImageWidth\": 4096,\\n \"supportedExtensions\": \"WMSServer\",\\n \"resampling\": false\\n}'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051b9d0-26f1-45f4-a07e-d5e0d5faf217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21cd8af3-8cf3-4263-9d5e-5e2cb0c3969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = requests.get(\"https://stackoverflow.com/questions/31126596/saving-response-from-requests-to-file\")\n",
    "with open(\"response.txt\", \"w\") as f:\n",
    "    f.write(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f8934-24d3-48b6-9038-0a5bff90fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = xxxxxxxxRequest(\n",
    "    data_collection=DataCollection.SENTINEL2_L2A,\n",
    "    layer=\"TRUE-COLOR-S2-L2A\",\n",
    "    bbox=bbox,\n",
    "    # time=\"2017-07-30\",\n",
    "    time=(\"2017-08-01\", \"2017-08-31\"),\n",
    "    width=2048,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f99e410c-ce03-4879-899c-604d9a1dae87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ncei' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13008\\4268623081.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m response = ncei.get_data(\n\u001b[0m\u001b[0;32m      2\u001b[0m       \u001b[0mdatasetid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"GHCND\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m       \u001b[0mstationid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"GHCND:USC00186350\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m       \u001b[0mdatatypeid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TMIN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TMAX\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0mstartdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"2015-12-01\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ncei' is not defined"
     ]
    }
   ],
   "source": [
    "response = ncei.get_data(\n",
    "      datasetid=\"GHCND\",\n",
    "      stationid=[\"GHCND:USC00186350\"],\n",
    "      datatypeid=[\"TMIN\", \"TMAX\"],\n",
    "      startdate=\"2015-12-01\",\n",
    "      enddate=\"2015-12-02\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c14209e8-a4e5-469d-83b2-33fcaacb3eac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arcgis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13008\\439386040.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0marcgis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0marcgis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'arcgis'"
     ]
    }
   ],
   "source": [
    "import arcgis\n",
    "from arcgis.gis import GIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497c72b-2146-48b4-bbfd-b4e7aa3a00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_GHCND_ACIS_Monthly/FeatureServer/0\n",
    "\n",
    "profile_name = \"christina\"\n",
    "\n",
    "gis = GIS(profile=profile_name)\n",
    "gis.users.me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb000f-4de4-4198-a89b-0b2c002aa152",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcnd_search(stationid = \"BE000006447\",\n",
    "               var = c(\"tmax\", \"tmin\"),\n",
    "               date_min = \"1839-01-01\",\n",
    "               date_max = \"1852-12-31\") %>%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d17f4e-3090-4e18-8355-deb1615e210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qe = f\"owner: {gis.users.me.username}\"\n",
    "my_content_count = gis.content.advanced_search(query=qe,return_count=True)\n",
    "print(my_content_count, 'items found for current user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac43e14-052c-4737-a48c-868f9b167c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_items = 3\n",
    "user_content = gis.content.advanced_search(query=qe, max_items=max_items)\n",
    "user_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534bc5e-bb2c-4add-8227-95dd80dbf718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff0d68-e5d2-47f4-a4ec-dec138577967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6a4ee-9ebb-4ab7-b2b7-7476e162edf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1f3bed8-baa5-40bb-9767-55c3427ea422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tools to access data from NOAA's Climate Data Online Web Services v2 API\"\"\"\n",
    "\n",
    "from copy import copy\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import urllib.parse\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NCEIBot:\n",
    "    \"\"\"Contains functions to request data from the NCEI web services\n",
    "\n",
    "    Attributes:\n",
    "        wait (float): time in seconds between requests. NCEI\n",
    "            allows a maximum of five queries per second.\n",
    "        validate_params (bool): whether to validate query parameters before\n",
    "            making a GET request. Defaults to False.\n",
    "        max_retries (int): number of times to retry requests that fail\n",
    "            because of temporary connectivity or server lapses. Retries\n",
    "            use an exponential backoff. Defaults to 12.\n",
    "\n",
    "    The get functions described below use a common set of keyword arguments.\n",
    "    The sortorder, limit, offset, and max arguments can be used in\n",
    "    any get function; other keywords vary by endpoint. Most values appear to\n",
    "    be case-sensitive. Query validation, if enabled, should capture\n",
    "    most but not all case errors.\n",
    "\n",
    "    Args:\n",
    "        datasetid (str or list): the id or name of a NCEI dataset. Multiple\n",
    "            values allowed for most functions. Examples: GHCND; PRECIP_HLY;\n",
    "            Weather Radar (Level III).\n",
    "        datacategoryid (str or list): the id or name of a NCEI data category.\n",
    "            Data categories are broader than data types. Multiple values\n",
    "            allowed. Examples: TEMP, WXTYPE, Degree Days.\n",
    "        datatypeid (str or list): the id or name of a data type. Multiple values\n",
    "            allowed. Examples: TMIN; SNOW; Long-term averages of fall growing\n",
    "            degree days with base 70F.\n",
    "        locationid (str or list): the id or name of a location. Multiple values\n",
    "            allowed. If a name is given, the script will try to map it to an id.\n",
    "            Examples: Maryland; FIPS:24; ZIP:20003; London, UK.\n",
    "        stationid (str or list): the id of name of a station in the NCEI\n",
    "            database. Multiple values allowed. Examples: COOP:010957.\n",
    "        startdate (str or datetime): the earliest date available\n",
    "        enddate (str or datetime): the latest date available\n",
    "        sortfield (str): field by which to sort the query results. Available\n",
    "            sort fields vary by endpoint.\n",
    "        sortorder (str): specifies whether sort is ascending or descending.\n",
    "            Must be 'asc' or 'desc'.\n",
    "        limit (int): number of records to return per query\n",
    "        offset (int): index of the first record to return\n",
    "        max (int): maximum number of records to return. Not part of the API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token, wait=0.2, cache_name=None, **cache_kwargs):\n",
    "        \"\"\"Initializes NCEIBot object\n",
    "\n",
    "        Args:\n",
    "            token (str): NCEI token\n",
    "            wait (float or int): time in seconds to wait between requests\n",
    "            cache_name (str): path to cache\n",
    "            cache_kwargs: any keyword argument accepted by requests_cache.CachedSession\n",
    "        \"\"\"\n",
    "\n",
    "        self.validate_params = False\n",
    "        self.max_retries = 12\n",
    "\n",
    "        # Queries are capped at five per second, so enforce that with\n",
    "        # a minimum wait time of 0.2 seconds\n",
    "        if wait < 0.2:\n",
    "            self.wait = 0.2\n",
    "        else:\n",
    "            self.wait = wait\n",
    "\n",
    "        if cache_kwargs and not cache_name:\n",
    "            raise Exception(\"Must specify cache_name if cache_kwargs are provided\")\n",
    "\n",
    "        # Cache queries using requests_cache\n",
    "        if cache_name:\n",
    "            self._cache = True\n",
    "            self._session = requests_cache.CachedSession(cache_name, **cache_kwargs)\n",
    "        else:\n",
    "            self._cache = False\n",
    "            self._session = requests.Session()\n",
    "\n",
    "        # Lazy load __version__ to prevent circular import error\n",
    "        # from . import __version__\n",
    "\n",
    "        self._session.headers.update(\n",
    "            {\"token\": token, \"User-Agent\": f\"pyncei\"}\n",
    "        )\n",
    "\n",
    "        self._validators = {\n",
    "            \"datacategoryid\": self._check_name,\n",
    "            \"datasetid\": self._check_name,\n",
    "            \"datatypeid\": self._check_name,\n",
    "            \"enddate\": self._check_date,\n",
    "            \"extent\": self._check_extent,\n",
    "            \"limit\": self._check_limit,\n",
    "            \"locationid\": self._check_name,\n",
    "            \"locationcategoryid\": self._check_name,\n",
    "            \"max\": self._check_positive_integer,\n",
    "            \"offset\": self._check_positive_integer,\n",
    "            \"stationid\": self._check_name,\n",
    "            \"startdate\": self._check_date,\n",
    "            \"sortfield\": self._check_sortfield,\n",
    "            \"sortorder\": self._check_sortorder,\n",
    "            \"units\": self._check_units,\n",
    "        }\n",
    "\n",
    "        # List of fields that can occur more than once in a given query.\n",
    "        # This list may need to be adjusted depending on the endpoint;\n",
    "        # for example, the data endpoint allows only one dataset to be passed.\n",
    "        self._allow_multiple = [\n",
    "            \"datacategoryid\",\n",
    "            \"datasetid\",\n",
    "            \"datatypeid\",\n",
    "            \"locationid\",\n",
    "            \"locationcategoryid\",\n",
    "            \"stationid\",\n",
    "        ]\n",
    "\n",
    "        # List of endpoints\n",
    "        self._endpoints = [\n",
    "            \"datacategories\",\n",
    "            \"datasets\",\n",
    "            \"datatypes\",\n",
    "            \"locations\",\n",
    "            \"locationcategories\",\n",
    "            \"stations\",\n",
    "        ]\n",
    "\n",
    "        # Create name lookups to help users map to ids needed for querying\n",
    "        self._lookups = {}\n",
    "        self._filepath = os.path.join(os.path.dirname(__file__), \"files\")\n",
    "        try:\n",
    "            os.makedirs(self._filepath)\n",
    "        except OSError:\n",
    "            for fp in glob.iglob(os.path.join(self._filepath, \"*.csv\")):\n",
    "                fn = os.path.splitext(os.path.basename(fp))[0]\n",
    "                self._lookups[fn] = {}\n",
    "                with open(fp, encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "                    rows = csv.reader(f, delimiter=\",\", quotechar='\"')\n",
    "                    try:\n",
    "                        next(rows)\n",
    "                    except StopIteration:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for row in rows:\n",
    "                            for item in row:\n",
    "                                self._lookups[fn][item.lower()] = tuple(row)\n",
    "\n",
    "    def get_data(self, **kwargs):\n",
    "        \"\"\"Retrieves historical climate data matching the given parameters\n",
    "\n",
    "        See :py:class:`~pyncei.bot.NCEIBot` for more details about each\n",
    "        keyword argument.\n",
    "\n",
    "        Args:\n",
    "            datasetid (str): Required. Only one value allowed.\n",
    "            startdate (str or datetime): Required. Returned stations will\n",
    "                have data for the specified dataset/type from on or after\n",
    "                this date.\n",
    "            enddate (str or datetime): Required. Returned stations will\n",
    "                have data for the specified dataset/type from on or before\n",
    "                this date.\n",
    "            datatypeid (str or list): Optional\n",
    "            locationid (str or list): Optional\n",
    "            stationid (str or list): Optional\n",
    "            units (str): Optional. One of 'standard' or 'metric'.\n",
    "            sortfield (str): Optional. If provided, must be one of 'datatype',\n",
    "                'date', or 'station'.\n",
    "            sortorder (str): Optional\n",
    "            limit (int): Optional\n",
    "            offset (int): Optional\n",
    "            max (int): Optional\n",
    "\n",
    "        Returns:\n",
    "            List of dicts containing historical weather data\n",
    "        \"\"\"\n",
    "        url = \"http://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n",
    "        required = [\"datasetid\", \"startdate\", \"enddate\", \"units\"]\n",
    "        optional = [\n",
    "            \"datatypeid\",\n",
    "            \"locationid\",\n",
    "            \"stationid\",\n",
    "            \"sortfield\",\n",
    "            \"sortorder\",\n",
    "            \"limit\",\n",
    "            \"offset\",\n",
    "            \"includemetadata\",\n",
    "        ]\n",
    "        # Assign default unit. Returned values are nonsense without this.\n",
    "        if not kwargs.get(\"units\"):\n",
    "            kwargs[\"units\"] = \"metric\"\n",
    "        self._allow_multiple.remove(\"datasetid\")\n",
    "        url, params = self._prepare_query(url, [], kwargs, required, optional)\n",
    "        self._allow_multiple.append(\"datasetid\")\n",
    "        return self._get(url, params)\n",
    "\n",
    "    def get_datasets(self, datasetid=None, **kwargs):\n",
    "        \"\"\"Returns data from the NCEI dataset endpoint\n",
    "\n",
    "        See :py:class:`~pyncei.bot.NCEIBot` for more details about each\n",
    "        keyword argument.\n",
    "\n",
    "        Args:\n",
    "            datasetid (str): a single dataset to return information about. Optional.\n",
    "                The kwargs are ignored if this is provided.\n",
    "            datatypeid (str or list): Optional\n",
    "            locationid (str or list): Optional\n",
    "            stationid (str or list): Optional\n",
    "            sortfield (str): Optional. If provided, must be one of 'id',\n",
    "                'name', 'mindate', 'maxdate', or 'datacoverage'.\n",
    "            sortorder (str): Optional\n",
    "            limit (int): Optional\n",
    "            offset (int): Optional\n",
    "            max (int): Optional\n",
    "\n",
    "        Returns:\n",
    "            List of dicts containing metadata for all matching datasets\n",
    "        \"\"\"\n",
    "        url = \"http://www.ncdc.noaa.gov/cdo-web/api/v2/datasets\"\n",
    "        required = []\n",
    "        optional = [\n",
    "            \"datatypeid\",\n",
    "            \"locationid\",\n",
    "            \"stationid\",\n",
    "            \"startdate\",\n",
    "            \"enddate\",\n",
    "            \"sortfield\",\n",
    "            \"sortorder\",\n",
    "            \"limit\",\n",
    "            \"offset\",\n",
    "        ]\n",
    "        url, params = self._prepare_query(url, datasetid, kwargs, required, optional)\n",
    "        return self._get(url, params)\n",
    "\n",
    "    def get_data_categories(self, datacategoryid=None, **kwargs):\n",
    "        \"\"\"Returns codes and labels for NCDI data categories\n",
    "\n",
    "        See :py:class:`~pyncei.bot.NCEIBot` for more details about each\n",
    "        keyword argument.\n",
    "\n",
    "        Args:\n",
    "            datacategoryid (str): a single data category to return information\n",
    "                about. Optional. The kwargs are ignored if this is provided.\n",
    "            datasetid (str or list): Optional\n",
    "            locationid (str or list): Optional\n",
    "            stationid (str or list): Optional\n",
    "            startdate (str or datetime): Optional\n",
    "            enddate (str or datetime): Optional\n",
    "            sortfield (str): Optional. If provided, must be one of 'id',\n",
    "                'name', 'mindate', 'maxdate', or 'datacoverage'.\n",
    "            sortorder (str): Optional\n",
    "            limit (int): Optional\n",
    "            offset (int): Optional\n",
    "            max (int): Optional\n",
    "\n",
    "        Returns:\n",
    "            List of dicts containing metadata for all matching data\n",
    "            categories\n",
    "        \"\"\"\n",
    "        url = \"http://www.ncdc.noaa.gov/cdo-web/api/v2/datacategories\"\n",
    "        required = []\n",
    "        optional = [\n",
    "            \"datasetid\",\n",
    "            \"locationid\",\n",
    "            \"stationid\",\n",
    "            \"startdate\",\n",
    "            \"enddate\",\n",
    "            \"sortfield\",\n",
    "            \"sortorder\",\n",
    "            \"limit\",\n",
    "            \"offset\",\n",
    "        ]\n",
    "        url, params = self._prepare_query(\n",
    "            url, datacategoryid, kwargs, required, optional\n",
    "        )\n",
    "        return self._get(url, params)\n",
    "\n",
    "    def get_data_types(self, datatypeid=None, **kwargs):\n",
    "        \"\"\"Returns information about NCEI data categories\n",
    "\n",
    "        See :py:class:`~pyncei.bot.NCEIBot` for more details about each\n",
    "        keyword argument.\n",
    "\n",
    "        Args:\n",
    "            datatypeid (str): a single data type to return information about.\n",
    "                Optional. The kwargs are ignored if this is provided.\n",
    "            datasetid (str or list): Optional\n",
    "            locationid (str or list): Optional\n",
    "            stationid (str or list): Optional\n",
    "            datacategoryid (str or list): Optional\n",
    "            startdate (str or datetime): Optional\n",
    "            enddate (str or datetime): Optional\n",
    "            sortfield (str): Optional. If provided, must be one of 'id',\n",
    "                'name', 'mindate', 'maxdate', or 'datacoverage'.\n",
    "            sortorder (str): Optional\n",
    "            limit (int): Optional\n",
    "            offset (int): Optional\n",
    "            max (int): Optional\n",
    "\n",
    "        Returns:\n",
    "            List of dicts containing metadata for all matching data types\n",
    "        \"\"\"\n",
    "        url = \"http://www.ncdc.noaa.gov/cdo-web/api/v2/datatypes\"\n",
    "        required = []\n",
    "        optional = [\n",
    "            \"datasetid\",\n",
    "            \"locationid\",\n",
    "            \"stationid\",\n",
    "            \"datacategoryid\",\n",
    "            \"startdate\",\n",
    "            \"enddate\",\n",
    "            \"sortfield\",\n",
    "            \"sortorder\",\n",
    "            \"limit\",\n",
    "            \"offset\",\n",
    "        ]\n",
    "        url, params = self._prepare_query(url, datatypeid, kwargs, required, optional)\n",
    "        return self._get(url, params)\n",
    "\n",
    "    def get_location_categories(self, locationcategoryid=None, **kwargs):\n",
    "        \"\"\"Returns information about NCEI location categories\n",
    "\n",
    "        See :py:class:`~pyncei.bot.NCEIBot` for more details about each\n",
    "        keyword argument.\n",
    "\n",
    "        Args:\n",
    "            locationcategoryid (str): a single location category to return\n",
    "                information about. Optional. The kwargs are ignored if this is\n",
    "                provided.\n",
    "            datasetid (str or list): Optional\n",
    "            sortfield (str): Optional. If provided, must be one of 'id' or\n",
    "                'name'.\n",
    "            sortorder (str): Optional\n",
    "            limit (int): Optional\n",
    "            offset (int): Optional\n",
    "            max (int): Optional\n",
    "\n",
    "        Returns:\n",
    "            List of dicts containing metadata about location categories\n",
    "        \"\"\"\n",
    "        url = \"http://www.ncdc.noaa.gov/cdo-web/api/v2/locationcategories\"\n",
    "        required = []\n",
    "        optional = [\n",
    "            \"datasetid\",\n",
    "            \"startdate\",\n",
    "            \"enddate\",\n",
    "            \"sortfield\",\n",
    "            \"sortorder\",\n",
    "            \"limit\",\n",
    "            \"offset\",\n",
    "        ]\n",
    "        url, params = self._prepare_query(\n",
    "            url, locationcategoryid, kwargs, required, optional\n",
    "        )\n",
    "        return self._get(url, params)\n",
    "\n",
    "    def get_locations(self, locationid=None, **kwargs):\n",
    "        \"\"\"Returns metadata for locations matching the given parameters\n",
    "\n",
    "        See :py:class:`~pyncei.bot.NCEIBot` for more details about each\n",
    "        keyword argument.\n",
    "\n",
    "        Args:\n",
    "            locationid (str): a single location to return information about.\n",
    "                Optional. The kwargs are ignored if this is provided.\n",
    "            datasetid (str or list): Optional\n",
    "            locationcategoryid (str or list): Optional\n",
    "            datacategoryid (str or list): Optional\n",
    "            sortfield (str): Optional. If provided, must be one of 'id',\n",
    "                'name', 'mindate', 'maxdate', or 'datacoverage'.\n",
    "            sortorder (str): Optional\n",
    "            limit (int): Optional\n",
    "            offset (int): Optional\n",
    "            max (int): Optional\n",
    "\n",
    "        Returns:\n",
    "            List of dicts containing metadata for all matching locations\n",
    "        \"\"\"\n",
    "        url = \"http://www.ncdc.noaa.gov/cdo-web/api/v2/locations\"\n",
    "        required = []\n",
    "        optional = [\n",
    "            \"datasetid\",\n",
    "            \"locationcategoryid\",\n",
    "            \"datacategoryid\",\n",
    "            \"startdate\",\n",
    "            \"enddate\",\n",
    "            \"sortfield\",\n",
    "            \"sortorder\",\n",
    "            \"limit\",\n",
    "            \"offset\",\n",
    "        ]\n",
    "        url, params = self._prepare_query(url, locationid, kwargs, required, optional)\n",
    "        return self._get(url, params)\n",
    "\n",
    "    def get_stations(self, stationid=None, **kwargs):\n",
    "        \"\"\"Returns metadata for stations matching the given parameters\n",
    "\n",
    "        See :py:class:`~pyncei.bot.NCEIBot` for more details about each\n",
    "        keyword argument.\n",
    "\n",
    "        Args:\n",
    "            stationid (str): a single station to return information about.\n",
    "                Optional. The kwargs are ignored if this is provided.\n",
    "            datasetid (str or list): Optional\n",
    "            locationid (str or list): Optional\n",
    "            datacategoryid (str or list): Optional\n",
    "            datatypeid (str or list): Optional\n",
    "            extent (str or iterable): comma-delimited bounding box of form\n",
    "                'min_lat, min_lng, max_lat, max_lng' or equivalent iterable.\n",
    "                Optional.\n",
    "            sortfield (str): Optional. If provided, must be one of 'id',\n",
    "                'name', 'mindate', 'maxdate', or 'datacoverage'.\n",
    "            sortorder (str): Optional\n",
    "            limit (int): Optional\n",
    "            offset (int): Optional\n",
    "            max (int): Optional\n",
    "\n",
    "        Returns:\n",
    "            List of dicts containing metadata for all matching stations\n",
    "        \"\"\"\n",
    "        url = \"http://www.ncdc.noaa.gov/cdo-web/api/v2/stations\"\n",
    "        required = []\n",
    "        optional = [\n",
    "            \"datasetid\",\n",
    "            \"locationid\",\n",
    "            \"datacategoryid\",\n",
    "            \"datatypeid\",\n",
    "            \"extent\",\n",
    "            \"startdate\",\n",
    "            \"enddate\",\n",
    "            \"sortfield\",\n",
    "            \"sortorder\",\n",
    "            \"limit\",\n",
    "            \"offset\",\n",
    "        ]\n",
    "        url, params = self._prepare_query(url, stationid, kwargs, required, optional)\n",
    "        return self._get(url, params)\n",
    "\n",
    "    def find_ids(self, term=None, endpoints=None):\n",
    "        \"\"\"Find key terms that match the search string for the given endpoints\n",
    "\n",
    "        Args:\n",
    "            term (str): the term to search for. If None, returns a list of all\n",
    "                available terms for the specified endpoint(s).\n",
    "            endpoints (str or list): name of one or more NCEI endpoints\n",
    "\n",
    "        Returns:\n",
    "            List of (endpoint, id, name) for matching key terms from the\n",
    "            specified endpoint\n",
    "        \"\"\"\n",
    "\n",
    "        if endpoints is None:\n",
    "            endpoints = sorted(self._lookups)\n",
    "        if isinstance(endpoints, str):\n",
    "            endpoints = [endpoints]\n",
    "\n",
    "        ids = []\n",
    "        for endpoint in endpoints:\n",
    "            try:\n",
    "                lookup = self._lookups[endpoint.lower()]\n",
    "            except KeyError:\n",
    "                raise\n",
    "            else:\n",
    "                try:\n",
    "                    matches = [lookup[term.lower()]]\n",
    "                except KeyError:\n",
    "                    matches = [v for k, v in lookup.items() if term.lower() in k]\n",
    "                ids.extend(sorted({(endpoint, *m) for m in matches}))\n",
    "        return ids\n",
    "\n",
    "    def refresh_lookups(self, keys=None):\n",
    "        \"\"\"Update the csv files used to populate the endpoint lookups\n",
    "\n",
    "        Args:\n",
    "            keys (list): list of endpoints to populate. If empty,\n",
    "                everything but stations will be populated.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        endpoints = {\n",
    "            \"datasets\": self.get_datasets,\n",
    "            \"datacategories\": self.get_data_categories,\n",
    "            \"datatypes\": self.get_data_types,\n",
    "            \"locationcategories\": self.get_location_categories,\n",
    "            \"locations\": self.get_locations,\n",
    "            \"stations\": self.get_stations,\n",
    "        }\n",
    "        if keys is None:\n",
    "            keys = [k for k in endpoints if k != \"stations\"]\n",
    "        elif not isinstance(keys, list):\n",
    "            keys = [keys]\n",
    "        for key in keys:\n",
    "            try:\n",
    "                response = endpoints[key]()\n",
    "            except KeyError as exc:\n",
    "                raise Exception(f\"{key} is not a valid id\") from exc\n",
    "            else:\n",
    "                fp = os.path.join(self._filepath, key + \".csv\")\n",
    "                with open(fp, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "                    writer = csv.writer(f, dialect=\"excel\")\n",
    "                    writer.writerow([\"id\", \"name\"])\n",
    "                    for result in response.values():\n",
    "                        row = [result[\"id\"], result[\"name\"]]\n",
    "                        writer.writerow(row)\n",
    "\n",
    "    def _get_with_retry(self, url, params):\n",
    "        \"\"\"Retries a get request with an exponential backoff\n",
    "\n",
    "        Args:\n",
    "            url (str): NCDI webservice url\n",
    "            params (dict): query parameters\n",
    "\n",
    "        Returns:\n",
    "            response to given request\n",
    "        \"\"\"\n",
    "        for i in range(self.max_retries):\n",
    "            try:\n",
    "                resp = self._session.get(url, params=self._encode_params(params))\n",
    "                # Retry if status code indicates a temporary problem\n",
    "                if resp.status_code in (429, 503):\n",
    "                    raise requests.exceptions.ConnectionError(\n",
    "                        f\"Request failed: {resp.url} (status_code={resp.status_code})\"\n",
    "                    )\n",
    "                return resp\n",
    "            except (\n",
    "                requests.exceptions.ConnectionError,\n",
    "                requests.exceptions.Timeout,\n",
    "            ) as err:\n",
    "                # Add a random number of milliseconds to the wait time to prevent\n",
    "                # multiple retries from synchronizing\n",
    "                wait = 2**i + random.randint(1, 1000) / 1000\n",
    "                print(\n",
    "                    f\"Retrying temporarily failed request in {wait}s\"\n",
    "                    f\" (url={url}, params={params}, error='{err}')\"\n",
    "                )\n",
    "                time.sleep(wait)\n",
    "        raise Exception(f\"Request failed (url={url}, params={params})\")\n",
    "\n",
    "    def _get(self, url, params):\n",
    "        \"\"\"Retrieves all matching records for a given url and parameter set\n",
    "\n",
    "        Args:\n",
    "            url (str): NCDI webservice url\n",
    "            params (dict): query parameters\n",
    "\n",
    "        Returns:\n",
    "            List of dicts containing the requested data\n",
    "        \"\"\"\n",
    "        # Many of the NCDI webservies have two different endpoints: one for\n",
    "        # a single, specific argument (for example, a station id), another\n",
    "        # for a query string. Here, specific requests are given a trailing\n",
    "        # backslash as a lazy way to tell the two types of reqeuests apart.\n",
    "        if not url.endswith(\"/\"):\n",
    "            try:\n",
    "                offset = params[\"offset\"]\n",
    "            except KeyError:\n",
    "                params[\"offset\"] = offset = 1\n",
    "            else:\n",
    "                # Offsets 0 and 1 both return the same record. Specifying\n",
    "                # an offset of 1 makes subsequent offsets (made by adding\n",
    "                # the limit to the last offset) start at the right record.\n",
    "                if not offset:\n",
    "                    params[\"offset\"] = offset = 1\n",
    "            # Minimize number of queries required to retrieve data\n",
    "            # by adjusting limit based on total number of records\n",
    "            try:\n",
    "                limit = params[\"limit\"]\n",
    "            except KeyError:\n",
    "                params[\"limit\"] = limit = 1000\n",
    "\n",
    "            try:\n",
    "                total = params.pop(\"max\")\n",
    "            except KeyError:\n",
    "                total = limit if limit < 1000 else 1e12  # any large number works\n",
    "            else:\n",
    "                if total < 1000:\n",
    "                    params[\"limit\"] = limit = total\n",
    "                else:\n",
    "                    params[\"limit\"] = limit = 1000\n",
    "\n",
    "        else:\n",
    "            total = limit = 1\n",
    "\n",
    "        logger.debug(\"Final parameter set:\")\n",
    "        if total > 0:\n",
    "            logger.debug(f\"total: {total}\")\n",
    "        for key in params:\n",
    "            logger.debug(f\"{key}: {params[key]}\")\n",
    "\n",
    "        response = NCEIResponse()\n",
    "        while response.count() < total:\n",
    "\n",
    "            logger.info(\"Requesting data\")\n",
    "\n",
    "            # NCEI does not like encoded colons, so encode the query string first\n",
    "            resp = self._get_with_retry(url, params)\n",
    "            if resp.status_code == 200:\n",
    "                logger.info(f\"Resolved {resp.url}\")\n",
    "\n",
    "                # Enforce a wait period between requests\n",
    "                if self._cache and not resp.from_cache:\n",
    "                    logger.info(\"Caching request\")\n",
    "                    time.sleep(self.wait)\n",
    "                elif not self._cache:\n",
    "                    logger.info(f\"Waiting {self.wait} seconds...\")\n",
    "                    time.sleep(self.wait)\n",
    "                else:\n",
    "                    logger.info(\"URL was retrieved from cache\")\n",
    "\n",
    "                response.append(resp)\n",
    "                if response.total() < total:\n",
    "                    total = response.total()\n",
    "                logger.info(f\"{response.count():,}/{total:,} records retrieved\")\n",
    "\n",
    "                try:\n",
    "                    params[\"offset\"] += limit\n",
    "                except KeyError:\n",
    "                    params[\"offset\"] = limit\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    f\"Failed to resolve {resp.url} ({resp.status_code}: {resp.text}\"\n",
    "                )\n",
    "        return response\n",
    "\n",
    "    def _prepare_query(self, url, endpoint_id, kwargs, required, optional):\n",
    "        \"\"\"Validate query\n",
    "\n",
    "        Args:\n",
    "            url (str): url to NCEI endpoint\n",
    "            endpoint_id (tuple): id from the endpoint\n",
    "            kwargs (dict): keyed query parameters\n",
    "            required (list): required fields for endpoint\n",
    "            optional (list): optional fields for endpoint\n",
    "\n",
    "        Returns:\n",
    "            Tuple (url string, paramter dict) if query is valid\n",
    "        \"\"\"\n",
    "        logger.info(f\"Preparing request to {url}\")\n",
    "        if endpoint_id:\n",
    "            if kwargs:\n",
    "                warnings.warn(f\"Ignoring kwargs: {kwargs}\")\n",
    "            # Return URL for a specific endpoint\n",
    "            return url + f\"/{endpoint_id}/\", {}\n",
    "        if self.validate_params:\n",
    "            # Extend optional with helper fields\n",
    "            optional.extend([\"max\"])\n",
    "            # Confirm that all required fields are present\n",
    "            missing = [key for key in required if not key in kwargs]\n",
    "            if missing:\n",
    "                raise Exception(f'Required parameters missing: {\", \".join(missing)}')\n",
    "            # Check that all fields in kwargs are valid\n",
    "            invalid = [key for key in kwargs if not key in required + optional]\n",
    "            if invalid:\n",
    "                raise Exception(f'Invalid parameters found: {\", \".join(invalid)}')\n",
    "            # Clean up kwargs\n",
    "            kwargs = self._check_kwargs(kwargs, url.split(\"/\").pop())\n",
    "        else:\n",
    "            # Try to map names to ids even if validation is disabled\n",
    "            ids = {\n",
    "                k: v\n",
    "                for k, v in kwargs.items()\n",
    "                if self._validators[k] in (self._check_name, self._check_extent)\n",
    "            }\n",
    "            kwargs.update(self._check_kwargs(ids, url.split(\"/\").pop()))\n",
    "        # Query string endpoint\n",
    "        return url, kwargs\n",
    "\n",
    "    def _check_kwargs(self, kwargs, endpoint):\n",
    "        \"\"\"Validates values given for query parameters\n",
    "\n",
    "        Args:\n",
    "            kwargs (dict): query parameters\n",
    "            endpoint (str): name of valid NCEI endpoint\n",
    "\n",
    "        Returns:\n",
    "            Dict containing cleaned up values for kwargs\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        # Check kwargs against validation functions\n",
    "        for key in kwargs.keys():\n",
    "            vals = kwargs[key]\n",
    "            # Extent can be an iterable, so treat lists in this key as one value\n",
    "            if isinstance(vals, (list, tuple)) and key == \"extent\":\n",
    "                vals = [vals]\n",
    "            if not isinstance(vals, (list, tuple)):\n",
    "                vals = [vals]\n",
    "            validated = []\n",
    "            for val in vals:\n",
    "                try:\n",
    "                    value, status = self._validators[key](val, key, endpoint)\n",
    "                except KeyError:\n",
    "                    # Catches bad parameter names. In practice, this should\n",
    "                    # never occur because bad params should be weeded out\n",
    "                    # beforehand.\n",
    "                    errors.append(f\"{key} is not a valid parameter\")\n",
    "                else:\n",
    "                    if status is False:\n",
    "                        errors.append(f\"{key}: {value} is invalid\")\n",
    "                    else:\n",
    "                        validated.append(value)\n",
    "                        logger.info(f\"{key}: {value} is valid\")\n",
    "\n",
    "            if not errors:\n",
    "\n",
    "                # Catch multiple values passed to key that only accepts one\n",
    "                if not key in self._allow_multiple:\n",
    "                    if len(validated) > 1:\n",
    "                        errors.append(f\"{key} only accepts one value\")\n",
    "                    else:\n",
    "                        validated = validated[0]\n",
    "\n",
    "                # Map helper fields to corresponding query fields\n",
    "                try:\n",
    "                    self._endpoints.index(re.sub(r\"id$\", \"\", key))\n",
    "                except ValueError:\n",
    "                    kwargs[key] = validated\n",
    "\n",
    "        if errors:\n",
    "            s = \"\" if len(errors) == 1 else \"s\"\n",
    "            raise Exception(f'Parameter error{s}: {\"; \".join(errors)}')\n",
    "\n",
    "        return kwargs\n",
    "\n",
    "    def _check_name(self, value, key, endpoint):\n",
    "        \"\"\"Map name to id for a given key, if possible\n",
    "\n",
    "        Args:\n",
    "            value (str): an identifer or name\n",
    "            key (str): name of field being checked\n",
    "            endpoint (str): name of current NCEI endpoint\n",
    "\n",
    "        Returns:\n",
    "            Tuple (id, True) if name is valid, or tuple\n",
    "            (error message, False) if not.\n",
    "        \"\"\"\n",
    "        endpoint = [e for e in self._endpoints if e.startswith(key.rstrip(\"deis\"))][0]\n",
    "        try:\n",
    "            ids = self.find_ids(value, endpoint)\n",
    "            if len(ids) == 1:\n",
    "                return ids[0][1], True\n",
    "        except KeyError:\n",
    "            # Allow original value through if no lookup is configured\n",
    "            warnings.warn(f\"No lookup list found for {endpoint}\")\n",
    "            return value, True\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        return f\"Failed to map '{value}' to an id\", False\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_date(date, key, endpoint):\n",
    "        \"\"\"Validate and formate date\n",
    "\n",
    "        Args:\n",
    "            date (str or dateime.datetime): date or equivalent\n",
    "            key (str): name of field being checked\n",
    "            endpoint (str): name of current NCEI endpoint\n",
    "\n",
    "        Returns:\n",
    "            Tuple (date string, True) if date is valid, or tuple\n",
    "            (error message, False) if not.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return date.strftime(\"%Y-%m-%d\"), True\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                datetime.strptime(date, \"%Y-%m-%d\")\n",
    "            except (TypeError, ValueError):\n",
    "                pass\n",
    "            else:\n",
    "                return date, True\n",
    "        return \"Must be a datetime object or string formatted as %Y-%m-%d\", False\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_extent(extent, key, endpoint):\n",
    "        \"\"\"Validate extent query parameter\n",
    "\n",
    "        Args:\n",
    "            extent (str or iterable): comma-delimited bounding box of form\n",
    "                'min_lat, min_lng, max_lat, max_lng' or equivalent iterable\n",
    "            key (str): name of field being checked\n",
    "            endpoint (str): name of current NCEI endpoint\n",
    "\n",
    "        Returns:\n",
    "            Tuple (extent string, True) if extent is valid, or tuple\n",
    "            (error message, False) if not.\n",
    "        \"\"\"\n",
    "        if isinstance(extent, str):\n",
    "            extent = [s.strip() for s in extent.split(\",\")]\n",
    "        min_lat, min_lng, max_lat, max_lng = [float(c) for c in extent]\n",
    "        if min_lat < max_lat and min_lng < max_lng:\n",
    "            return \",\".join([str(s) for s in extent]), True\n",
    "        return 'Must be string/iterable of \"min_lat, min_lng, max_lat, max_lng\"', False\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_sortfield(value, key, endpoint):\n",
    "        \"\"\"Validate sortfield query parameter\n",
    "\n",
    "        Args:\n",
    "            value (str): name of sort field. Sort fields vary by endpoint.\n",
    "            key (str): name of field being checked\n",
    "            endpoint (str): name of current NCEI endpoint\n",
    "\n",
    "        Returns:\n",
    "            Tuple (sort field, True) if sort field is valid, or tuple\n",
    "            (error message, False) if not.\n",
    "        \"\"\"\n",
    "        fields = {\n",
    "            \"data\": [\"datatype\", \"date\", \"station\"],\n",
    "            \"datasets\": [\"id\", \"name\", \"mindate\", \"maxdate\", \"datacoverage\"],\n",
    "            \"datacategories\": [\"id\", \"name\"],\n",
    "            \"locationcategories\": [\"id\", \"name\"],\n",
    "            \"locations\": [\"id\", \"name\", \"mindate\", \"maxdate\", \"datacoverage\"],\n",
    "            \"stations\": [\"id\", \"name\", \"mindate\", \"maxdate\", \"datacoverage\"],\n",
    "        }\n",
    "        try:\n",
    "            value = value.lower()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            if value in fields[endpoint]:\n",
    "                return value, True\n",
    "        return f'Must be one of the following: {\", \".join(fields[endpoint])}', False\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_sortorder(value, key, endpoint):\n",
    "        \"\"\"Validate sort order\n",
    "\n",
    "        Args:\n",
    "            value (str): 'asc' or 'desc'\n",
    "            key (str): name of field being checked\n",
    "            endpoint (str): name of current NCEI endpoint\n",
    "\n",
    "        Returns:\n",
    "            Tuple (validated string, True) if order is valid, or tuple\n",
    "            (error message, False) if not.\n",
    "        \"\"\"\n",
    "        valid = [\"asc\", \"desc\"]\n",
    "        try:\n",
    "            value = value.lower()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            if value in valid:\n",
    "                return value, True\n",
    "        return f'Must be one of the following: {\", \".join(valid)}', False\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_units(value, key, endpoint):\n",
    "        \"\"\"Validate units\n",
    "\n",
    "        Args:\n",
    "            value (str): 'standard' or 'metric'\n",
    "            key (str): name of field being checked\n",
    "            endpoint (str): name of current NCEI endpoint\n",
    "\n",
    "        Returns:\n",
    "            Tuple (validated string, True) if order is valid, or tuple\n",
    "            (error message, False) if not.\n",
    "        \"\"\"\n",
    "        valid = [\"standard\", \"metric\"]\n",
    "        try:\n",
    "            value = value.lower()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            if value in valid:\n",
    "                return value, True\n",
    "        return f'Must be one of the following: {\", \".join(valid)}', False\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_limit(value, key, endpoint):\n",
    "        \"\"\"Validate limit\n",
    "\n",
    "        Args:\n",
    "            value (str or int): integer to validate\n",
    "            key (str): name of field being checked\n",
    "            endpoint (str): name of current NCEI endpoint\n",
    "\n",
    "        Returns:\n",
    "            Tuple (validated integer, True) if limit is valid, or tuple\n",
    "            (error message, False) if not.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except (TypeError, ValueError):\n",
    "            pass\n",
    "        else:\n",
    "            if 0 < value <= 1000:\n",
    "                return value, True\n",
    "        return \"Must be an integer between 1 and 1000, inclusive\", False\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_positive_integer(value, key, endpoint):\n",
    "        \"\"\"Validate positive integer\n",
    "\n",
    "        Args:\n",
    "            value (str or int): integer to validate\n",
    "            key (str): name of field being checked\n",
    "            endpoint (str): name of current NCEI endpoint\n",
    "\n",
    "        Returns:\n",
    "            Tuple (validated integer, True) if number is valid, or tuple\n",
    "            (error message, False) if not.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except (TypeError, ValueError):\n",
    "            pass\n",
    "        else:\n",
    "            if value >= 0:\n",
    "                return value, True\n",
    "        return \"Must be an integer greater than or equal to 0\", False\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_params(params, safe=\":,\"):\n",
    "        param_list = []\n",
    "        for key, vals in params.items():\n",
    "            for val in vals if isinstance(vals, (list, tuple)) else [vals]:\n",
    "                param_list.append((key, val))\n",
    "        return urllib.parse.urlencode(param_list, safe=safe)\n",
    "\n",
    "\n",
    "class NCEIResponse(list):\n",
    "    \"\"\"Wraps results of one or more calls to the NCEI API\n",
    "\n",
    "    Extends list. Each response is stored as an entry in the list.\n",
    "    \"\"\"\n",
    "\n",
    "    #: list used to order the keys in the NCEI data\n",
    "    key_order = [\n",
    "        \"id\",\n",
    "        \"uid\",\n",
    "        \"name\",\n",
    "        \"station\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"elevation\",\n",
    "        \"elevationUnit\",\n",
    "        \"datacoverage\",\n",
    "        \"date\",\n",
    "        \"mindate\",\n",
    "        \"maxdate\",\n",
    "        \"datatype\",\n",
    "        \"attributes\",\n",
    "        \"value\",\n",
    "        \"url\",\n",
    "        \"retrieved\",\n",
    "    ]\n",
    "\n",
    "    #: dict mapping NCEI fields to date formats\n",
    "    date_formats = {\n",
    "        \"date\": \"%Y-%m-%dT%H:%M:%S\",\n",
    "        \"maxdate\": \"%Y-%m-%d\",\n",
    "        \"mindate\": \"%Y-%m-%d\",\n",
    "        \"retrieved\": \"%Y-%m-%dT%H:%M:%S\",\n",
    "    }\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"<{self.__class__.__name__} responses={len(self)}\"\n",
    "            f\" count={self.count()} total={self.total()}>\"\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def __bool__(self):\n",
    "        for resp in self:\n",
    "            if resp.json():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def values(self):\n",
    "        \"\"\"Gets the results from all responses\n",
    "\n",
    "        Returns:\n",
    "            generator of dicts\n",
    "        \"\"\"\n",
    "        for resp in self:\n",
    "            metadata = {\n",
    "                \"url\": resp.url,\n",
    "                \"retrieved\": datetime.strptime(\n",
    "                    resp.headers[\"Date\"], \"%a, %d %b %Y %H:%M:%S %Z\"\n",
    "                ).isoformat(),\n",
    "            }\n",
    "            for val in self._get_results(resp):\n",
    "                if val:\n",
    "                    val.update(metadata)\n",
    "\n",
    "                keys = set(val.keys())\n",
    "                if keys - set(self.key_order):\n",
    "                    raise KeyError(\n",
    "                        f\"Found unordered keys: {keys - set(self.key_order)}\"\n",
    "                    )\n",
    "\n",
    "                yield {k: val[k] for k in self.key_order if k in keys}\n",
    "\n",
    "    def first(self):\n",
    "        \"\"\"Gets the first result from the compiled responses\n",
    "\n",
    "        Returns:\n",
    "            dict\n",
    "        \"\"\"\n",
    "        for val in self.values():\n",
    "            return val\n",
    "\n",
    "    def count(self):\n",
    "        \"\"\"Counts the number of results that have been returned\n",
    "\n",
    "        Returns:\n",
    "            number of records returned as int\n",
    "        \"\"\"\n",
    "        return sum([len(self._get_results(r)) for r in self])\n",
    "\n",
    "    def total(self):\n",
    "        \"\"\"Counts the total number of results available for all URLs\n",
    "\n",
    "        Returns:\n",
    "            total number of records matching the responses as int\n",
    "        \"\"\"\n",
    "        urls = {}\n",
    "        for resp in self:\n",
    "            # Group by url with pagination parameters removed\n",
    "            url = re.sub(r\"\\b(offset|limit|max)=\\d+\\b\", \"&\", resp.url).strip(\"&\")\n",
    "            try:\n",
    "                urls.setdefault(url, int(resp.json()[\"metadata\"][\"resultset\"][\"count\"]))\n",
    "            except KeyError:\n",
    "                urls.setdefault(url, 1)\n",
    "        return sum(urls.values())\n",
    "\n",
    "    def to_csv(self, path):\n",
    "        \"\"\"Writes data to a CSV\n",
    "\n",
    "        Args:\n",
    "            path (str): path to csv\n",
    "        \"\"\"\n",
    "        with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            writer = csv.writer(f, dialect=\"excel\")\n",
    "            keys = None\n",
    "            for row in self.values():\n",
    "                row = row.copy()\n",
    "                if not keys:\n",
    "                    keys = row.keys()\n",
    "                    writer.writerow(keys)\n",
    "                writer.writerow([row[k] for k in keys])\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        \"\"\"Writes data to a dataframe\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame or geopandas.GeoDataFrame if geopandas is installed\n",
    "            and the responses include coordinates\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self.values())\n",
    "\n",
    "        # Convert datetime columns to datetime objects\n",
    "        for key, date_format in self.date_formats.items():\n",
    "            if key in df.columns:\n",
    "                df[key] = pd.to_datetime(df[key], format=date_format)\n",
    "\n",
    "        # Convert DataFrame with coordinates to GeoDataFrame if geopandas installed.\n",
    "        # Uses NAD83 as the CRS. This appears to be NOAA's preferred CRS but it's\n",
    "        # not explicitly defined in the webservice documentation that I could find.\n",
    "        if \"latitude\" in df.columns and \"longitude\" in df.columns:\n",
    "            try:\n",
    "                df = gpd.GeoDataFrame(\n",
    "                    df,\n",
    "                    geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
    "                    crs=\"NAD83\",\n",
    "                )\n",
    "            except NameError:\n",
    "                # geopandas is optional\n",
    "                pass\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_results(resp):\n",
    "        resp_json = resp.json()\n",
    "        try:\n",
    "            return resp_json[\"results\"]\n",
    "        except KeyError:\n",
    "            return [resp_json]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa2b002c-eb7f-4d69-b7e3-ded46a39f238",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13008\\2807042173.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mncei\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNCEIBot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ExampleNCEIAPIToken\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13008\\3411399902.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, token, wait, cache_name, **cache_kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;31m# Create name lookups to help users map to ids needed for querying\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lookups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"files\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "ncei = NCEIBot(\"ExampleNCEIAPIToken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ddeeaff-4010-4fe3-910e-37ade4772668",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13008\\2030215925.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m response = get_data(\n\u001b[0m\u001b[0;32m      2\u001b[0m       \u001b[0mdatasetid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"GHCND\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m       \u001b[0mstationid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"GHCND:USC00186350\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m       \u001b[0mdatatypeid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TMIN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TMAX\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0mstartdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"2015-12-01\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "response = ncei.get_data(\n",
    "      datasetid=\"GHCND\",\n",
    "      stationid=[\"GHCND:USC00186350\"],\n",
    "      datatypeid=[\"TMIN\", \"TMAX\"],\n",
    "      startdate=\"2015-12-01\",\n",
    "      enddate=\"2015-12-02\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e69434-7db0-41ee-b3ef-f00db5a91948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
