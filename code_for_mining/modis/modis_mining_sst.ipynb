{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATIONS_DIR = \"../../locs/\"\n",
    "DATA_DIR = \"../../datasets/modis/sst/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geojson_context_figure(files: list[str]):\n",
    "    ## plot the geojson regions over a world map for checking\n",
    "    world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))  # type: ignore\n",
    "\n",
    "    for file in files:\n",
    "        gdf = gpd.read_file(file)  # load the region shape\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        world.plot(ax=ax, color=\"lightgrey\")\n",
    "        gdf.plot(ax=ax, edgecolor=\"red\", facecolor=\"none\")\n",
    "        ax.set_axis_off()\n",
    "        output_path = file.replace(\".geojson\", \".png\")\n",
    "        plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(region_name, data, latitude, longitude, start_date, end_date):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # Set vmax to 0.75 for colorbar maximum\n",
    "    plt.pcolormesh(longitude, latitude, data, shading=\"auto\", vmin=0, vmax=0.75)\n",
    "    plt.colorbar(label=\"Chlorophyll-a concentration\", extend=\"max\")\n",
    "    plt.title(f\"Region: {region_name}\\n{start_date} to {end_date}\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.savefig(f\"./plots/{region_name}/{start_date}_{end_date}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_mean(\n",
    "    region_name: str,\n",
    "    mean_values: dict[str, float],\n",
    "    ylabel: str = \"default ylabel\",\n",
    "    title: str = \"default title\",\n",
    "    interval: int = 1,\n",
    "):\n",
    "    # Create a new figure for the time series plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "    # Extract dates and mean values\n",
    "    dates = [\n",
    "        datetime.strptime(date_str[:8], \"%Y%m%d\") for date_str in mean_values.keys()\n",
    "    ]\n",
    "    values = list(mean_values.values())\n",
    "\n",
    "    # Plot the time series\n",
    "    ax.plot_date(dates, values, fmt=\"-\")\n",
    "\n",
    "    # Set axis labels and title\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Rotate x-axis labels for better visibility\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Adjust x-axis tick locator to show dates\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=interval))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./plots/{region_name}/{title.replace(' ', '_')}_mean.png\")\n",
    "    plt.close()\n",
    "    return dates, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(data_series, region_name):\n",
    "    dates = [info[\"start_date\"] for info in data_series]\n",
    "    # Ensure that the data is copied from the original dataset to a writable array\n",
    "    means = [\n",
    "        np.nanmean(np.array(info[\"data\"]))\n",
    "        for info in data_series\n",
    "        if info[\"data\"].size > 0\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(dates, means, marker=\"o\", linestyle=\"-\")\n",
    "    plt.title(f\"Time Series of Mean Chlorophyll-a Concentration for {region_name}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Mean Chlorophyll-a Concentration\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./plots/{region_name}/time_series.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data_series, region_name):\n",
    "    try:\n",
    "        valid_data_series = [\n",
    "            d\n",
    "            for d in data_series\n",
    "            if \"start_date\" in d and \"end_date\" in d and d[\"data\"].size > 0\n",
    "        ]\n",
    "\n",
    "        if not valid_data_series:\n",
    "            print(f\"No valid data to process for {region_name}.\")\n",
    "            return\n",
    "\n",
    "        # Define columns as just the months in a single year\n",
    "        months = [\n",
    "            \"Jan\",\n",
    "            \"Feb\",\n",
    "            \"Mar\",\n",
    "            \"Apr\",\n",
    "            \"May\",\n",
    "            \"Jun\",\n",
    "            \"Jul\",\n",
    "            \"Aug\",\n",
    "            \"Sep\",\n",
    "            \"Oct\",\n",
    "            \"Nov\",\n",
    "            \"Dec\",\n",
    "        ]\n",
    "\n",
    "        # Create an empty DataFrame with years as index and months as columns\n",
    "        years = sorted(set([d[\"start_date\"].year for d in valid_data_series]))\n",
    "        df = pd.DataFrame(index=years, columns=months)\n",
    "\n",
    "        for entry in valid_data_series:\n",
    "            data_copy = np.array(entry[\"data\"]).copy()\n",
    "            mean_value = (\n",
    "                np.nanmean(data_copy) if np.any(~np.isnan(data_copy)) else np.nan\n",
    "            )\n",
    "            month = entry[\"start_date\"].strftime(\"%b\")\n",
    "            year = entry[\"start_date\"].year\n",
    "            # Set the mean value at the correct year and month\n",
    "            df.at[year, month] = mean_value\n",
    "\n",
    "        # Remove any rows that are entirely NaN\n",
    "        df.dropna(how=\"all\", inplace=True)\n",
    "\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(f\"./data_csv/{region_name}\", exist_ok=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_path = (\n",
    "            f\"./data_csv/{region_name}/chlorophyll_monthly_means_{region_name}.csv\"\n",
    "        )\n",
    "        df.to_csv(csv_path, index_label=\"year\")\n",
    "        print(f\"CSV file created for {region_name} at {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process data for {region_name}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir_path: str, file_type: str):\n",
    "    # find all the files in a directory\n",
    "    files = [\n",
    "        os.path.join(dir_path, file)\n",
    "        for file in os.listdir(dir_path)\n",
    "        if file.endswith(\".\" + file_type)\n",
    "    ]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_952046/4044039121.py:3: FutureWarning: The geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n",
      "  world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "## import the GeoJSON files which contains the coordinates of the region(s) of interest\n",
    "# find all the geojson files in a directory\n",
    "\n",
    "locs_files = get_files(LOCATIONS_DIR, \"geojson\")\n",
    "geojson_context_figure(locs_files)\n",
    "\n",
    "# read the GeoJSON files\n",
    "gdf_list = [gpd.read_file(file) for file in locs_files]\n",
    "\n",
    "# set the limits of the region(s) of interest with lists for each min and max\n",
    "x_min_list = [gdf.total_bounds[0] for gdf in gdf_list]\n",
    "y_min_list = [gdf.total_bounds[1] for gdf in gdf_list]\n",
    "x_max_list = [gdf.total_bounds[2] for gdf in gdf_list]\n",
    "y_max_list = [gdf.total_bounds[3] for gdf in gdf_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data files available: 263\n"
     ]
    }
   ],
   "source": [
    "data_files = get_files(DATA_DIR, \"nc\")\n",
    "data_files.sort()\n",
    "\n",
    "# parse the start and end date information from the filename (AQUA_MODIS.20210101_20210131.L3m.MO.CHL.chlor_a.4km.nc) of each file and convert it to a datetime object\n",
    "start_date = [\n",
    "    datetime.strptime(file.split(\"/\")[-1].split(\".\")[1].split(\"_\")[0], \"%Y%m%d\")\n",
    "    for file in data_files\n",
    "]\n",
    "end_date = [\n",
    "    datetime.strptime(file.split(\"/\")[-1].split(\".\")[1].split(\"_\")[1], \"%Y%m%d\")\n",
    "    for file in data_files\n",
    "]\n",
    "\n",
    "# create a list of dictionaries containing the filename, start date and end date for each file\n",
    "data_files_info = [\n",
    "    {\"filename\": file, \"start_date\": start, \"end_date\": end}\n",
    "    for file, start, end in zip(data_files, start_date, end_date)\n",
    "]\n",
    "\n",
    "# specify the start and end dates for the desired date range\n",
    "start_date_range = datetime(2000, 1, 1)\n",
    "end_date_range = datetime.now()  # datetime(2022, 12, 31)\n",
    "\n",
    "# print the the number of data files available\n",
    "print(f\"Total data files available: {len(data_files_info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected date range: 2000-01-01 00:00:00 to 2024-09-19 16:03:53.564343\n",
      "Total data files available within the date range: 263\n"
     ]
    }
   ],
   "source": [
    "# filter the data_files_info list based on the date range\n",
    "filtered_files_info = [\n",
    "    file_info\n",
    "    for file_info in data_files_info\n",
    "    if start_date_range <= file_info[\"start_date\"] <= end_date_range\n",
    "    and start_date_range <= file_info[\"end_date\"] <= end_date_range\n",
    "]\n",
    "\n",
    "print(f\"Selected date range: {start_date_range} to {end_date_range}\")\n",
    "print(f\"Total data files available within the date range: {len(filtered_files_info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading global data: 100%|████████████████████████████████████████████████████████████████████████████████████| 263/263 [00:01<00:00, 234.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful loads: 263\n",
      "Unsuccessful loads: 0\n",
      "Size of data loaded: 2200 bytes (~0.00 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "successful_loads = 0\n",
    "unsuccessful_loads = 0\n",
    "data_list = []\n",
    "\n",
    "# read the data\n",
    "for file_info in tqdm(filtered_files_info, desc=\"Loading global data\"):\n",
    "    try:\n",
    "        data = nc.Dataset(file_info[\"filename\"], \"r\")  # type: ignore\n",
    "        successful_loads += 1\n",
    "        data_list.append(\n",
    "            {\n",
    "                \"data\": data,\n",
    "                \"start_date\": file_info[\"start_date\"],\n",
    "                \"end_date\": file_info[\"end_date\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        unsuccessful_loads += 1\n",
    "\n",
    "print(f\"Successful loads: {successful_loads}\")\n",
    "print(f\"Unsuccessful loads: {unsuccessful_loads}\")\n",
    "print(\n",
    "    f\"Size of data loaded: {sys.getsizeof(data_list)} bytes (~{(sys.getsizeof(data_list) / 1024**3):.2f} GB)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = {\n",
    "    gdf_name: []\n",
    "    for gdf_name in [\n",
    "        os.path.basename(locs_file).replace(\".geojson\", \"\") for locs_file in locs_files\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping data to region of interest:   0%|                                                                               | 0/263 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping data to region of interest: 100%|█████████████████████████████████████████████████████████████████████| 263/263 [02:29<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# loop through all the data, and crop it to the region of interest\n",
    "for data_info in tqdm(data_list, desc=\"Cropping data to region of interest\"):\n",
    "    latitude = data_info[\"data\"][\"lat\"][:]\n",
    "    longitude = data_info[\"data\"][\"lon\"][:]\n",
    "    # chlor_a = data_info[\"data\"][\"chlor_a\"][:]\n",
    "    sst = data_info[\"data\"]['sst'][:]\n",
    "    # replace fill values with NaN for better plotting\n",
    "    # fill_value = data_info[\"data\"][\"chlor_a\"]._FillValue\n",
    "    fill_value = data_info[\"data\"]['sst']._FillValue\n",
    "    sst[sst == fill_value] = np.nan\n",
    "    sst[sst < 0] = np.nan\n",
    "    # chlor_a[chlor_a == fill_value] = np.nan\n",
    "    # chlor_a[chlor_a < 0] = np.nan  # set any data below 0 to NaN\n",
    "\n",
    "    # loop through each region\n",
    "    for i, gdf in enumerate(gdf_list):\n",
    "        sst_crop = sst[\n",
    "            (latitude >= y_min_list[i]) & (latitude <= y_max_list[i]), :\n",
    "        ]\n",
    "        sst_crop = sst_crop[\n",
    "            :, (longitude >= x_min_list[i]) & (longitude <= x_max_list[i])\n",
    "        ]\n",
    "        longitude_crop = longitude[\n",
    "            (longitude >= x_min_list[i]) & (longitude <= x_max_list[i])\n",
    "        ]\n",
    "        latitude_crop = latitude[\n",
    "            (latitude >= y_min_list[i]) & (latitude <= y_max_list[i])\n",
    "        ]\n",
    "        region_name = os.path.basename(locs_files[i]).replace(\".geojson\", \"\")\n",
    "        time_series_data[region_name].append(\n",
    "            {\n",
    "                \"data\": sst_crop,\n",
    "                \"latitude\": latitude_crop,\n",
    "                \"longitude\": longitude_crop,\n",
    "                \"start_date\": data_info[\"start_date\"],\n",
    "                \"end_date\": data_info[\"end_date\"],\n",
    "                \"region_name\": region_name,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cropped and time series plotted.\n"
     ]
    }
   ],
   "source": [
    "for region_name, data_series in time_series_data.items():\n",
    "\n",
    "    os.makedirs(f\"./plots/{region_name}\", exist_ok=True)\n",
    "    # plot_time_series(data_series, region_name)\n",
    "    # write_to_csv(data_series, region_name)\n",
    "\n",
    "del data_list\n",
    "print(\"Data cropped and time series plotted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_mean(\n",
    "    region_name: str,\n",
    "    mean_values: dict[str, float],\n",
    "    ylabel: str = \"default ylabel\",\n",
    "    title: str = \"default title\",\n",
    "    interval: int = 1,\n",
    "):\n",
    "    # Create a new figure for the time series plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "    # Extract dates and mean values\n",
    "    dates = [\n",
    "        datetime.strptime(date_str[:8], \"%Y%m%d\") for date_str in mean_values.keys()\n",
    "    ]\n",
    "    values = list(mean_values.values())\n",
    "\n",
    "    # Plot the time series\n",
    "    ax.plot_date(dates, values, fmt=\"-\")\n",
    "\n",
    "    # Set axis labels and title\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Rotate x-axis labels for better visibility\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Adjust x-axis tick locator to show dates\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=interval))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./plots/{region_name}/{title.replace(' ', '_')}_mean.png\")\n",
    "    plt.close()\n",
    "    return dates, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in time_series_data.keys():\n",
    "    mean_values: dict[str, float] = {}\n",
    "    for data_info in time_series_data[region]:\n",
    "        time_period = f\"{data_info['start_date'].strftime('%Y%m%d')}_{data_info['end_date'].strftime('%Y%m%d')}\"\n",
    "        # print the number of nans in the data\n",
    "        nans = np.isnan(data_info[\"data\"]).sum()\n",
    "        total = data_info[\"data\"].size\n",
    "        print(\n",
    "            f\"Region: {region}, time period: {time_period}, NaNs: {nans}, Total: {total}, % NaNs: {nans / total * 100:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Calculate mean value\n",
    "        mean_value = float(np.mean(data_info[\"data\"]))\n",
    "\n",
    "        # Store mean value in dictionary\n",
    "        mean_values[time_period] = mean_value\n",
    "\n",
    "    os.makedirs(f\"./plots/{region}\", exist_ok=True)\n",
    "    dates, values = plot_data_mean(\n",
    "        region_name=f\"{region}\",\n",
    "        mean_values=mean_values,\n",
    "        ylabel=\"temp\",\n",
    "        title=f\"{region.replace('_', ' ')} sea surface temp over time\",\n",
    "        interval=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Nino_3.4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Calculate ONI for Niño 3.4 region\u001b[39;00m\n\u001b[1;32m     52\u001b[0m nino_region_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNino_3.4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 53\u001b[0m oni_series \u001b[38;5;241m=\u001b[39m calculate_oni(nino_region_name, \u001b[43mtime_series_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnino_region_name\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Plot the ONI\u001b[39;00m\n\u001b[1;32m     56\u001b[0m plot_oni(oni_series, nino_region_name)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Nino_3.4'"
     ]
    }
   ],
   "source": [
    "# %% [ONI Calculation and Plotting]\n",
    "def calculate_oni(region_name, data_series, window_size=36):\n",
    "    \"\"\"Calculate the Oceanic Niño Index (ONI) for the specified region.\n",
    "    The ONI is calculated as a rolling average of SST anomalies.\n",
    "    \n",
    "    Args:\n",
    "        region_name (str): The name of the region (e.g., 'Nino_3.4').\n",
    "        data_series (list): A list of dictionaries containing the SST data for the region.\n",
    "        window_size (int): The number of months over which to calculate the rolling average.\n",
    "                           Default is 36 months (3 years).\n",
    "\n",
    "    Returns:\n",
    "        pandas.Series: The ONI values indexed by date.\n",
    "    \"\"\"\n",
    "    # Extract dates and mean SST values for the region\n",
    "    dates = [info[\"start_date\"] for info in data_series]\n",
    "    means = [np.nanmean(info[\"data\"]) for info in data_series]\n",
    "\n",
    "    # Create a pandas DataFrame with SST values indexed by date\n",
    "    sst_df = pd.DataFrame(data={'SST': means}, index=pd.to_datetime(dates))\n",
    "\n",
    "    # Calculate the rolling mean (anomalies are typically calculated against a baseline)\n",
    "    rolling_mean_sst = sst_df['SST'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "    # Calculate SST anomalies by subtracting the rolling mean from the SST values\n",
    "    anomalies = sst_df['SST'] - rolling_mean_sst\n",
    "\n",
    "    return anomalies\n",
    "\n",
    "def plot_oni(oni_series, region_name):\n",
    "    \"\"\"Plot the Oceanic Niño Index (ONI).\n",
    "    \n",
    "    Args:\n",
    "        oni_series (pandas.Series): The ONI values indexed by date.\n",
    "        region_name (str): The name of the region (e.g., 'Nino_3.4').\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(oni_series.index, oni_series, marker='o', linestyle='-')\n",
    "    plt.axhline(0, color='gray', linestyle='--', linewidth=1)  # Zero anomaly line\n",
    "    plt.axhline(0.5, color='red', linestyle='--', linewidth=1)  # El Niño threshold\n",
    "    plt.axhline(-0.5, color='blue', linestyle='--', linewidth=1)  # La Niña threshold\n",
    "    plt.title(f'Oceanic Niño Index (ONI) for {region_name}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('SST Anomaly (°C)')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./plots/{region_name}/oni_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Calculate ONI for Niño 3.4 region\n",
    "nino_region_name = \"Nino_3.4\"\n",
    "oni_series = calculate_oni(nino_region_name, time_series_data[nino_region_name])\n",
    "\n",
    "# Plot the ONI\n",
    "plot_oni(oni_series, nino_region_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
